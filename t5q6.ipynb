{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d88fce-4df9-4daf-ba0c-d94e6d544c9c",
   "metadata": {},
   "source": [
    "# EE2211 Tutorial 5\n",
    "\n",
    "### by Chua Dingjuan (Sep 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d08d32-df84-4abd-8f6c-728a4d88a431",
   "metadata": {},
   "source": [
    "## Question 6 Python Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc512376-9dca-48c4-935e-11eeaf5e55db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599 entries, 0 to 1598\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   fixed acidity         1599 non-null   float64\n",
      " 1   volatile acidity      1599 non-null   float64\n",
      " 2   citric acid           1599 non-null   float64\n",
      " 3   residual sugar        1599 non-null   float64\n",
      " 4   chlorides             1599 non-null   float64\n",
      " 5   free sulfur dioxide   1599 non-null   float64\n",
      " 6   total sulfur dioxide  1599 non-null   float64\n",
      " 7   density               1599 non-null   float64\n",
      " 8   pH                    1599 non-null   float64\n",
      " 9   sulphates             1599 non-null   float64\n",
      " 10  alcohol               1599 non-null   float64\n",
      " 11  quality               1599 non-null   int64  \n",
      "dtypes: float64(11), int64(1)\n",
      "memory usage: 150.0 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "## get data from web \n",
    "wine = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\",sep=';')\n",
    "wine.info()\n",
    "y = wine.quality\n",
    "x = wine.drop('quality',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cd19641-6095-4bd9-895d-4580995fb74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.   ,  7.4  ,  0.7  , ...,  3.51 ,  0.56 ,  9.4  ],\n",
       "       [ 1.   ,  7.8  ,  0.88 , ...,  3.2  ,  0.68 ,  9.8  ],\n",
       "       [ 1.   ,  7.8  ,  0.76 , ...,  3.26 ,  0.65 ,  9.8  ],\n",
       "       ...,\n",
       "       [ 1.   ,  6.3  ,  0.51 , ...,  3.42 ,  0.75 , 11.   ],\n",
       "       [ 1.   ,  5.9  ,  0.645, ...,  3.57 ,  0.71 , 10.2  ],\n",
       "       [ 1.   ,  6.   ,  0.31 , ...,  3.39 ,  0.66 , 11.   ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Include the offset/bias term\n",
    "# In this method we are creating a columns of ones (x0) and we horizontally stack (hstack) it with x\n",
    "\n",
    "x0 = np.ones((len(y),1))\n",
    "X = np.hstack((x0,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b34e62c-e9c4-4d4b-847d-bda0e157e428",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split data into training and test sets\n",
    "## (Note: this exercise introduces the basic protocol of using the training-test\n",
    "## partitioning of samples for evaluation assuming the list of data is already randomly indexed)\n",
    "\n",
    "## In case you really want a general random split to have a better training/test distributions:\n",
    "## from sklearn.model_selection import train_test_split\n",
    "## train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=99/1599, random_state= 0)\n",
    "\n",
    "train_X = X[0:1500]\n",
    "train_y = y[0:1500]\n",
    "test_X = X[1500:1599]\n",
    "test_y = y[1500:1599]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb8ec5-fd9b-4e40-b5f8-b1f85be45dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## linear regression\n",
    "w = inv(train_X.T @ train_X) @ train_X.T @ train_y\n",
    "print(w)\n",
    "\n",
    "yt_est = test_X.dot(w);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf35125-ce16-4e7a-a26e-f50adba88923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following two codes are different ways of doing the same thing!\n",
    "\n",
    "# MSE --> Find the differences between two variables, square all the errors, take the mean!\n",
    "\n",
    "MSE = np.square(np.subtract(test_y,yt_est)).mean()\n",
    "print(MSE)\n",
    "\n",
    "MSE = mean_squared_error(test_y,yt_est)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa69e6b-5808-427d-8ba3-9f579217697f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
